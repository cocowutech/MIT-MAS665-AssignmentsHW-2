# Adaptive English Placement Agent

FastAPI backend with a minimal web UI for login and a Gemini-powered test call. Includes product spec endpoints and idle auto-shutdown.

## Prerequisites (WSL Ubuntu)
- Python 3.12
- `python3-venv` installed (Ubuntu: `sudo apt-get install -y python3-venv`)

## Setup
```bash
cd ~/MIT-MAS665-AssignmentsHW-2
python3 -m venv .venv
. .venv/bin/activate
pip install -r requirements.txt
```

## Configure environment (.env)
Create or edit `.env` in the project root:
```bash
# Required
GEMINI_API_KEY=YOUR_KEY
GEMINI_MODEL=gemini-2.5-flash

# Provider: choose ONE
GEMINI_PROVIDER=vertex            # Vertex AI Express (recommended for AQ.* keys)
# or
# GEMINI_PROVIDER=ai_studio       # Google AI Studio (often keys start with AIza)

# Vertex settings (only if using provider=vertex)
GEMINI_VERTEX_REGION=us-central1
GEMINI_VERTEX_PROJECT=YOUR_GCP_PROJECT_ID

# Auth (seed user and JWT)
SEED_USERNAME=rong_wu
SEED_PASSWORD=mit!23456
JWT_SECRET_KEY=change-this-in-prod

# Idle shutdown (seconds with no requests before exit)
IDLE_SHUTDOWN_SECONDS=1200
```

Notes:
- For Vertex Express, enable Vertex AI API in `YOUR_GCP_PROJECT_ID` and ensure the API key is allowed for Vertex.
- For AI Studio, set `GEMINI_PROVIDER=ai_studio` and use a Studio key.

## Run the server
```bash
./scripts/run.sh
# or run in background (what the setup used):
./scripts/run.sh >/tmp/placement_api.log 2>&1 & echo $! > /tmp/placement_api.pid
```

Visit:
- Frontend: `http://127.0.0.1:8000/app`
- Health: `http://127.0.0.1:8000/health`
- Root: `http://127.0.0.1:8000/`

### Module UIs
- Reading page: `http://127.0.0.1:8000/app/read/`
  - Model: Gemini 2.0 Flash-Lite (passage + question generation)
  - Flow: 3 passages × 5 questions (15 total), starts at A2, adaptive A1–C2
- Writing page: `http://127.0.0.1:8000/app/write/`
  - Model: Gemini 2.5 Flash-Lite (prompt generation and scoring)
- Vocabulary page: `http://127.0.0.1:8000/app/vocabulary/`
  - Model: Gemini 2.5 Flash / Flash-Lite
- Speaking page: `http://127.0.0.1:8000/app/speaking/`
  - Model: Gemini 2.5 Flash-Lite

## Login and try Gemini
Default seed user (from `.env`):
- username: `rong_wu`
- password: `mit!23456`

## Writing module

- Endpoints (JWT required):
  - `POST /write/prompt` → body `{ "band": "A2|B1|B2|C1", "topic?": string }` → returns `{ prompt, band, exam_task, targets }` where targets include `target_vocab` and `target_structures` aligned to KET/PET/FCE.
  - `POST /write/score/text` → body `{ "text": string, "band_hint?": "A2|B1|B2|C1" }` → returns JSON with `band`, `exam_mapping`, `scores` (content, organization, language_control, range), `overall`, `word_count`, `comments` (global + inline annotations).
  - `POST /write/score/image` → multipart form with `file` (image) and optional `band_hint` → runs OCR then scores as above.

- Frontend at `/app/write/` supports:
  - Login → Generate prompt aligned to band (auto-generated by Gemini)
  - Type text and score OR upload an image (OCR + score)
  - Displays band estimate, rubric sub-scores, and inline comments

### OCR setup

Install system dependency for OCR (WSL/Ubuntu):
```bash
sudo apt-get update && sudo apt-get install -y tesseract-ocr
```

Python deps are in `requirements.txt`: `pytesseract`, `Pillow`, `python-multipart`.

Via frontend:
- Open `http://127.0.0.1:8000/app`, log in, and press Generate.

Via API (curl):
```bash
# Get token
curl -s -X POST http://127.0.0.1:8000/auth/token \
  -H 'Content-Type: application/x-www-form-urlencoded' \
  -d 'username=rong_wu&password=mit!23456'

# Use token to call Gemini
TOKEN=... # paste access_token from previous step
curl -s -X POST http://127.0.0.1:8000/gemini/generate \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer $TOKEN" \
  -d '{"prompt":"Say hello in one short line."}'
```

## Product spec endpoints
- JSON: `GET /spec`
- Markdown: `GET /spec/markdown`

## Idle auto-shutdown
- The server exits if no requests are received for `IDLE_SHUTDOWN_SECONDS` (default 1200s).

## Troubleshooting
- `gemini_configured: false` on `/`: ensure `.env` is in project root and server started from the root (the provided `scripts/run.sh` already does this).
- 401 from Gemini:
  - AI Studio: verify `GEMINI_PROVIDER=ai_studio` and the key is a Studio key.
  - Vertex: set `GEMINI_PROVIDER=vertex`, set `GEMINI_VERTEX_PROJECT`, enable Vertex AI API, and ensure the key is permitted for Vertex Express.
- Login fails: confirm `SEED_USERNAME` / `SEED_PASSWORD` in `.env` and restart the server.

## Project layout
```
backend/
  app/
    routers/
      auth.py        # OAuth2 password flow, JWT
      gemini.py      # Protected /gemini/generate
      health.py      # /health
      spec.py        # /spec, /spec/markdown
    gemini_client.py # Vertex/AI Studio client wrapper
    main.py          # FastAPI app, static mount, idle shutdown
    settings.py      # Pydantic settings from .env
frontend/
  index.html         # Minimal UI (login + generate)
scripts/
  run.sh             # Launch Uvicorn with .venv
requirements.txt
.env                 # Your secrets (gitignored)
```

